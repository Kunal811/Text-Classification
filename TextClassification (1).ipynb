{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =\"C:/Users/kunal/OneDrive/Desktop/20_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "content = load_files(data, encoding=\"utf-8\", decode_error=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split of content data and target fields\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(content.data, content.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 14997)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test), len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will return top 500 frequent words which will be chosen as features of the dataset\n",
    "def Features(X_train):\n",
    "    #dictionary to obtain feature_set\n",
    "    feature_set ={}\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    for doc in X_train:\n",
    "        #remove all stopwords, numbers, special symbols from each documents\n",
    "        removedNumber = re.sub(r'[0-9]+', ' ', doc)\n",
    "        cleanString = re.sub(r\"[^a-zA-Z0-9]+\", ' ', removedNumber)\n",
    "        clean = re.sub(r'\\b\\w{1,3}\\b', ' ', cleanString)\n",
    "        \n",
    "        #tokenized each document\n",
    "        word_tokens = word_tokenize(clean) \n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        for word in filtered_sentence:\n",
    "            \n",
    "            #check whether the word is present in dict and increase its freq by 1 if already present\n",
    "            #intialize with 1 if not present\n",
    "            if word not in feature_set:\n",
    "                feature_set[word] = 1\n",
    "            else:\n",
    "                feature_set[word] += 1\n",
    "    \n",
    "    #sort the dictionary in reverse order to get most frequent words\n",
    "    data = []\n",
    "    features = []\n",
    "    for keys, values in feature_set.items():\n",
    "        data.append((values,keys))\n",
    "    data.sort(reverse=True)\n",
    "    print(data[0])\n",
    "    \n",
    "    #select top 500 words as features\n",
    "    for i in range(0,500):\n",
    "        features.append(data[i][1])\n",
    "        \n",
    "    return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function converts dataset into a 2-D array with columns as features and rows corresponding to each document\n",
    "\n",
    "def DataSet(data, features):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #define a numpy array as data_x\n",
    "    data_x = np.zeros((len(data),len(features)))\n",
    "    i = 0\n",
    "    #we will go through each document\n",
    "    for document in data:\n",
    "        #remove all stopwords, numbers, special symbols from each documents\n",
    "        \n",
    "        removedNumber = re.sub(r'[0-9]+', ' ', document)\n",
    "        str1 = re.sub(r\"[^a-zA-Z0-9]+\", ' ', removedNumber)\n",
    "        str2 = re.sub(r'\\b\\w{1,3}\\b', ' ', str1)\n",
    "        word_tokens = word_tokenize(str2) \n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        #for each word in the document\n",
    "        for word in filtered_sentence:\n",
    "            \n",
    "            #go through each word in filtered_sentence and convert data\n",
    "            #in numpy array according to frequency of word in document\n",
    "            for word2, j in zip(features, range(len(features))):\n",
    "                if word == word2:\n",
    "                    data_x[i][j] += 1\n",
    "                    break\n",
    "        i += 1\n",
    "        \n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit function for obtaining the sum of frequencies of all the words\n",
    "#in each class of Y_train\n",
    "def fit(X1, Y_train, feature):\n",
    "    #count is dictionary for all values of unique_class\n",
    "    count = {}\n",
    "    #unique classes \n",
    "    class_value = set(Y_train)\n",
    "    for unique_class in class_value:\n",
    "        #for each class create dictionary for keeping the sum\n",
    "        #of each words in a feature set\n",
    "        count[unique_class] = {}\n",
    "        count[\"total_count\"] = len(Y_train)\n",
    "        current_class_rows = (Y_train == unique_class)\n",
    "        X_train_current = X1[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        #total_count1 is the count of Y_train belonging to a particular unique_class  \n",
    "        count[unique_class][\"total_count1\"] = len(Y_train_current)\n",
    "        count[unique_class][\"total_data\"] = X_train_current[:,:].sum()\n",
    "        \n",
    "        for i in range(1,len(feature)+1):\n",
    "            #for each feature obtain sum of all the frequencies in the X_train_current row \n",
    "            count[unique_class][i] = X_train_current[:,i-1].sum()\n",
    "            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probability function to return probability value for each class\n",
    "def probability(dictionary, x, current_class):\n",
    "    #inital output value for each class\n",
    "    output = np.log(dictionary[current_class][\"total_count1\"]) - np.log(dictionary[\"total_count\"])\n",
    "    number = len(dictionary[current_class].keys())-2\n",
    "    #for loop over all features  \n",
    "    for j in range(1,number+1):\n",
    "        #xj for getting if the word is part of the feature_set\n",
    "        xj = x[j-1]\n",
    "        \n",
    "        # prob formula= (number of that word in each class)/(sum of all the words in that class) \n",
    "       \n",
    "        \n",
    "        if xj != 0:\n",
    "             #do laplace correction\n",
    "            count_current_class_with_value_xj = dictionary[current_class][j] + 1\n",
    "            count_current_class = dictionary[current_class][\"total_data\"] + len(dictionary[current_class].keys()) - 2\n",
    "            \n",
    "            #use log probability\n",
    "            current_xj_probablity = np.log(count_current_class_with_value_xj) - np.log(count_current_class)\n",
    "            \n",
    "            #add output values to obtain final probability value for each class\n",
    "            output = output + current_xj_probablity\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictSinglePoint function that return the best class for a single row x\n",
    "def predictSinglePoint(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if (current_class == \"total_count\"):\n",
    "            continue\n",
    "        #for each class call probability function to obtain probability values\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        #compare probability value obtain with best_p and assign the new value\n",
    "        #to best_p\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict function that return the predicited values for X_test\n",
    "def predict(dictionary, X_test):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        #for each X in X_test call predictSinglePoint\n",
    "        x_class = predictSinglePoint(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22904, 'news')\n"
     ]
    }
   ],
   "source": [
    "#feature_set obtain from X_train \n",
    "features = Features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting X_train into a 2D numpy array\n",
    "X1_train = DataSet(X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 1.],\n",
       "       [3., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [3., 2., 1., ..., 0., 0., 0.],\n",
       "       [2., 1., 1., ..., 0., 0., 0.],\n",
       "       [3., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting X_test into a 2D  numpy array\n",
    "X2_test = DataSet(X_test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call fit function\n",
    "dictionary = fit(X1_train,Y_train, features)\n",
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict function to predict text \n",
    "Y_pred1 = predict(dictionary,X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.70      0.76       243\n",
      "           1       0.90      0.72      0.80       260\n",
      "           2       0.75      0.92      0.83       257\n",
      "           3       0.64      0.57      0.60       261\n",
      "           4       0.64      0.73      0.68       259\n",
      "           5       0.86      0.69      0.76       245\n",
      "           6       0.76      0.95      0.85       244\n",
      "           7       0.86      0.89      0.88       263\n",
      "           8       0.79      0.94      0.86       248\n",
      "           9       0.92      0.96      0.94       251\n",
      "          10       0.94      0.94      0.94       243\n",
      "          11       0.96      0.90      0.93       263\n",
      "          12       0.83      0.89      0.86       278\n",
      "          13       0.88      0.63      0.73       257\n",
      "          14       0.88      0.88      0.88       238\n",
      "          15       0.98      0.99      0.98       243\n",
      "          16       0.71      0.94      0.81       226\n",
      "          17       0.96      0.83      0.89       229\n",
      "          18       0.74      0.66      0.70       247\n",
      "          19       0.68      0.67      0.68       245\n",
      "\n",
      "    accuracy                           0.82      5000\n",
      "   macro avg       0.82      0.82      0.82      5000\n",
      "weighted avg       0.82      0.82      0.82      5000\n",
      "\n",
      "[[169   0   0   0   0   0   2   2  15   1   0   0   2   1   0   0   2   0\n",
      "    0  49]\n",
      " [  0 186  15  11  15   8  11   1   1   0   0   0   8   1   3   0   0   0\n",
      "    0   0]\n",
      " [  0   1 237   6   0  10   3   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   9 148  77   4   6   0   0   0   0   5   6   0   2   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0  52 190   1   7   1   1   0   0   0   4   0   2   0   0   0\n",
      "    0   0]\n",
      " [  0   7  51   6   1 168   6   0   1   0   0   0   1   2   2   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   2   1   0 232   5   2   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   9 235   5   2   0   0   5   2   1   1   1   0\n",
      "    2   0]\n",
      " [  0   0   0   0   0   1   1   6 233   2   1   0   0   2   0   2   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   1   0 240   8   0   0   1   0   1   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   1   0   9 229   0   0   2   1   0   0   0\n",
      "    0   0]\n",
      " [  1   1   3   1   3   1   3   1   2   2   1 236   4   1   1   0   2   0\n",
      "    0   0]\n",
      " [  1   1   1   6   4   1   5   3   3   0   0   1 247   2   3   0   0   0\n",
      "    0   0]\n",
      " [  4   4   0   0   4   2  12  15  28   5   3   2  11 161   3   0   2   1\n",
      "    0   0]\n",
      " [  0   2   0   0   2   0   5   1   2   1   0   0   6   6 210   0   0   1\n",
      "    2   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0   0   0   0 241   0   0\n",
      "    0   1]\n",
      " [  0   0   0   0   0   0   0   1   2   0   0   1   0   0   0   0 213   0\n",
      "    9   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  16 189\n",
      "   22   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0  11   1  42   5\n",
      "  162  25]\n",
      " [ 29   0   0   0   0   0   1   0   0   0   0   0   1   2   0   1  23   1\n",
      "   22 165]]\n"
     ]
    }
   ],
   "source": [
    "##classification report using inbulid sklearn MultinomialNB()\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X1_train, Y_train)\n",
    "Y_pred = clf.predict(X2_test)\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       243\n",
      "           1       0.91      0.67      0.77       260\n",
      "           2       0.77      0.89      0.83       257\n",
      "           3       0.67      0.60      0.64       261\n",
      "           4       0.63      0.76      0.69       259\n",
      "           5       0.84      0.73      0.78       245\n",
      "           6       0.76      0.95      0.85       244\n",
      "           7       0.92      0.92      0.92       263\n",
      "           8       0.78      0.97      0.86       248\n",
      "           9       0.96      0.96      0.96       251\n",
      "          10       0.95      0.96      0.96       243\n",
      "          11       0.98      0.91      0.94       263\n",
      "          12       0.81      0.94      0.87       278\n",
      "          13       0.79      0.65      0.71       257\n",
      "          14       0.94      0.84      0.89       238\n",
      "          15       1.00      0.99      0.99       243\n",
      "          16       0.65      0.96      0.77       226\n",
      "          17       0.97      0.77      0.86       229\n",
      "          18       0.71      0.62      0.66       247\n",
      "          19       0.74      0.51      0.60       245\n",
      "\n",
      "    accuracy                           0.82      5000\n",
      "   macro avg       0.83      0.82      0.81      5000\n",
      "weighted avg       0.83      0.82      0.81      5000\n",
      "\n",
      "[[186   1   0   0   0   0   2   2  19   1   0   0   2   4   0   0   1   0\n",
      "    0  25]\n",
      " [  0 173  13  12  20   9  13   0   0   0   0   0  13   7   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1 230   7   0  11   6   0   0   0   0   1   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   5 157  84   3   3   0   0   0   0   2   2   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0  45 198   1   4   1   0   0   0   0   7   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   7  47   5   2 178   3   0   1   0   0   1   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   3   1   0 232   4   1   0   0   0   2   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   7 241   3   0   0   0   7   2   0   0   2   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   1   1   2 240   0   0   0   1   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   0   0   0 241   8   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   1   2   5 233   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   2   0   0   3   4   1   1   2   0 239   4   3   0   0   4   0\n",
      "    0   0]\n",
      " [  0   1   0   3   2   1   6   2   1   0   0   0 260   0   2   0   0   0\n",
      "    0   0]\n",
      " [  4   2   0   1   4   2  12   9  33   1   3   0  17 166   2   0   1   0\n",
      "    0   0]\n",
      " [  0   2   0   0   1   2   6   0   5   0   0   0   5  16 200   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0   0   0   0 241   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 216   0\n",
      "   10   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0  25 176\n",
      "   25   2]\n",
      " [  1   0   0   0   0   0   1   0   0   0   0   0   0   2   7   0  59   6\n",
      "  154  17]\n",
      " [ 63   0   0   0   0   0   2   0   0   0   0   1   1   0   0   1  26   0\n",
      "   26 125]]\n"
     ]
    }
   ],
   "source": [
    "##classification_report for predicted values using naive bayes code\n",
    "print(classification_report(Y_test,Y_pred1))\n",
    "print(confusion_matrix(Y_test,Y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
